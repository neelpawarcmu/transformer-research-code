{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchtext import datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot histogram of data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocd_data = torch.load(\"artifacts/saved_data/preprocd_data.pt\").numpy()\n",
    "sent_pairs, languages, max_len = preprocd_data.shape\n",
    "aggregated_data = preprocd_data.reshape([sent_pairs * languages, max_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_token_hist(data):\n",
    "\n",
    "    # Count occurrences of values equal to 2 and not equal to 2\n",
    "    count_equal_to_2 = np.sum(data == 2)\n",
    "    count_not_equal_to_2 = np.sum(data != 2)\n",
    "\n",
    "    # Plot histogram with two bars\n",
    "    plt.bar(['Token=2', 'Token!=2'], [count_equal_to_2, count_not_equal_to_2], color=['blue', 'orange'])\n",
    "    plt.xlabel('Entry')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Histogram of Token=2 and Token!=2')\n",
    "    plt.gca().get_yaxis().get_major_formatter().set_scientific(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_token_hist(aggregated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonpad_token_count = np.sum(preprocd_data != 2)\n",
    "pad_token_count = np.sum(preprocd_data == 2)\n",
    "total_token_count = pad_token_count + nonpad_token_count\n",
    "percent_nonpad = nonpad_token_count / total_token_count * 100\n",
    "print(f\"nonpad_token_count: {nonpad_token_count}\\n\"\n",
    "      f\"pad_token_count: {pad_token_count}\\n\"\n",
    "      f\"total_token_count: {total_token_count}\\n\"\n",
    "      f\"percent_nonpad: {percent_nonpad:2f} %\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonpad_sums = np.sum(aggregated_data != 2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_sentence = nonpad_sums.max()\n",
    "shortest_sentence = nonpad_sums.min()\n",
    "avg_sentence = np.mean(nonpad_sums)\n",
    "print(f\"longest_sentence: {longest_sentence} tokens\\n\"\n",
    "      f\"shortest_sentence: {shortest_sentence} tokens\\n\"\n",
    "      f\"avg_sentence: {avg_sentence:2f} tokens\\n\"\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### histogram of sentence lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sentence_lens(data, title):\n",
    "    plt.imshow(data != 2, aspect='auto')\n",
    "    plt.xlabel(\"Sentence lengths\")\n",
    "    plt.ylabel(\"Sentence number\")\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sentence_lens(preprocd_data[:10000, 0, :], \"German sentences\")\n",
    "plot_sentence_lens(preprocd_data[:10000, 1, :], \"English sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### histogram of sentence lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonpad_token_counts = np.count_nonzero(preprocd_data != 2, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ger_nonpad_token_counts = nonpad_token_counts[:,0]\n",
    "eng_nonpad_token_counts = nonpad_token_counts[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ger_nonpad_token_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, sharey=True, tight_layout=True)\n",
    "# We can set the number of bins with the *bins* keyword argument.\n",
    "counts, bins, _ = axs[0].hist(ger_nonpad_token_counts, bins=range(50))\n",
    "print(f'counts: {counts}')\n",
    "axs[1].hist(eng_nonpad_token_counts, bins=range(50))\n",
    "axs[0].set_title('German non-pad token counts')\n",
    "axs[1].set_title('English non-pad token counts')\n",
    "axs[0].set_xlabel('Non-pad token counts')\n",
    "axs[0].set_ylabel(f'Number of sentences (total {len(ger_nonpad_token_counts)})')\n",
    "axs[1].set_xlabel('Non-pad token counts')\n",
    "axs[1].set_ylabel(f'Number of sentences (total {len(eng_nonpad_token_counts)})')\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_bleu_N1 = []\n",
    "validation_bleu_N1 = []\n",
    "with open('training_history_N1.txt') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        if \"Epoch:\" in line:\n",
    "            parts = line.split(\"|\")\n",
    "            tbleu = float(parts[1].split(\"BLEU: \")[-1].strip())\n",
    "            vbleu = float(parts[2].split(\"BLEU: \")[-1].strip())\n",
    "            training_bleu_N1.append(tbleu)\n",
    "            validation_bleu_N1.append(vbleu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_bleu_N1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(training_bleu_N1, label='training_bleu_N1')\n",
    "plt.plot(validation_bleu_N1, label='validation_bleu_N1')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_bleu_N6 = []\n",
    "validation_bleu_N6 = []\n",
    "with open('training_history_N6.txt') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        if \"Epoch:\" in line:\n",
    "            parts = line.split(\"|\")\n",
    "            tbleu = float(parts[1].split(\"BLEU: \")[-1].strip())\n",
    "            vbleu = float(parts[2].split(\"BLEU: \")[-1].strip())\n",
    "            training_bleu_N6.append(tbleu)\n",
    "            validation_bleu_N6.append(vbleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(training_bleu_N6, label='training_bleu_N6')\n",
    "plt.plot(validation_bleu_N6, label='validation_bleu_N6')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = datasets.IWSLT2016()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = load_dataset('wmt14', 'de-en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(d['test']['translation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in d['test']['translation']:\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t,v,te = d['train'], d['validation'], d['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in dl:\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd = [tuple(sentence_pair.values()) for sentence_pair in t['translation'] + v['translation'] + te['translation']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in rd:\n",
    "    print(elem)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.functional import to_map_style_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence_pair in rd:\n",
    "    print(sentence_pair)\n",
    "    src_sentence, tgt_sentence = sentence_pair.values()\n",
    "    print(src_sentence)\n",
    "    print(tgt_sentence)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_map = to_map_style_dataset(rd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in data_map:\n",
    "    print(elem)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
